{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Explained\n",
    "\n",
    "This notebook is used evaluate the best model for the dataset, perform hyperparameter tuning and cross validation for our chosen model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import KFold, StratifiedKFold \n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "df = pd.read_json('feature_eng_dataset.json')\n",
    "X = df.loc[:, ~df.columns.isin(['Outcome', 'Home_Win'])]\n",
    "y = df['Home_Win']\n",
    "\n",
    "X = X.drop([\n",
    "    'Season',\n",
    "    'Elo_home_avg_3',\n",
    "    'Elo_home_avg_10',\n",
    "    'Elo_away_avg_3',\n",
    "    'Elo_away_avg_10',\n",
    "    'Home_Goals_avg_3',\n",
    "    'Away_Goals_avg_3',\n",
    "    'Home_Defence',\n",
    "    'Away_Defence',\n",
    "    'Home_Streak',\n",
    "    'Away_Streak',\n",
    "    'Home_Team_Outcome_sum_3',\n",
    "    'Away_Team_Outcome_sum_3',\n",
    "    'Home_Goals_avg_10',\n",
    "    'Away_Goals_avg_10'   \n",
    "    ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split and standardise\n",
    "def split_datasets(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(models, X_train, y_train, y_test):\n",
    "    for model in models:\n",
    "        model[1].fit(X_train, y_train)\n",
    "        y_pred = model[1].predict(X_test)\n",
    "        accu = accuracy_score(y_test, y_pred) * 100\n",
    "        print(\n",
    "            f\"{model[0]}: \"\n",
    "            f\"Accuracy: {accu:.2f}\"\n",
    "            )\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lgr: Accuracy: 60.50\n",
      "rfc: Accuracy: 58.46\n",
      "knn: Accuracy: 56.42\n",
      "dtc: Accuracy: 54.05\n",
      "abc: Accuracy: 60.25\n",
      "gbc: Accuracy: 60.55\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2)\n",
    "\n",
    "models = [\n",
    "    ('lgr', make_pipeline(StandardScaler(), LogisticRegression())),\n",
    "    ('rfc', RandomForestClassifier(max_depth=2)),\n",
    "    ('knn', make_pipeline(StandardScaler(), KNeighborsClassifier())),\n",
    "    ('dtc', DecisionTreeClassifier()),\n",
    "    ('abc', AdaBoostClassifier()),\n",
    "    ('gbc', GradientBoostingClassifier())\n",
    "    ]\n",
    "\n",
    "X_train, X_test, y_train, y_test = split_datasets(X, y)\n",
    "compare_models(models, X_train, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning for Logistic Regression with Grid Search\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = LogisticRegression()\n",
    "params = {\n",
    "    'solver': ['newton-cg', 'lbfgs', 'liblinear'],\n",
    "    'penalty': ['none', 'l1', 'l2', 'elasticnet'],\n",
    "    'C': [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100]}\n",
    "\n",
    "cv = KFold(n_splits=10, random_state=1, shuffle=True)\n",
    "grid_search = GridSearchCV(model_1, params, scoring='accuracy', n_jobs=-1, cv=cv)\n",
    "grid_result = grid_search.fit(X_train, y_train)\n",
    "print(f'Best Score: {grid_result.best_score_ * 100:.2f}%')\n",
    "print(f'Best Hyperparameters: {grid_result.best_params_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Best Score: 60.93%\n",
    "    Best Hyperparameters: {'C': 0.001, 'penalty': 'l2', 'solver': 'lbfgs'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verfiy Grid Search with Random Search method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import uniform\n",
    "\n",
    "distributions = dict(C=uniform(loc=0, scale=4),\n",
    "                    penalty=['l2', 'l1'])\n",
    "\n",
    "ran_search = RandomizedSearchCV(model_1, params, n_iter = 20, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "ran_result = ran_search.fit(X_train, y_train)\n",
    "print(f'Best score: {ran_result.best_score_ * 100:.2f}%')\n",
    "print(f'Best params: {ran_result.best_params_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Best score: 60.88%\n",
    "    Best params: {'solver': 'lbfgs', 'penalty': 'l2', 'C': 0.001}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning for Gradient Boosting \n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "[CV] END .................................................... total time=   8.8s\n",
      "[CV] END .................................................... total time=   8.9s\n",
      "[CV] END .................................................... total time=   9.3s\n",
      "Best Score: 60.87%\n",
      "Best Hyperparameters: {}\n"
     ]
    }
   ],
   "source": [
    "n_estimators = [int(x) for x in np.linspace(start = 10, stop = 200, num = 20)]\n",
    "min_split = [int(x) for x in np.linspace(start = 100, stop = 500, num = 4)]\n",
    "max_depth = [int(x) for x in np.linspace(2, 32, num = 4)]\n",
    "min_leaf = [int(x) for x in np.linspace(start = 1, stop = 50, num = 5)]\n",
    "max_features = [int(x) for x in np.linspace(start = 7, stop = 10, num = 7)]\n",
    "\n",
    "grid = {#'n_estimators': n_estimators,\n",
    "#        'min_samples_split': min_split,\n",
    "#        'max_depth': max_depth,\n",
    "#        'min_samples_leaf': min_leaf,\n",
    "#        'max_features': max_features,\n",
    "#        'subsample':[0.6,0.7,0.75,0.8,0.85,0.9,1.0],\n",
    "        }\n",
    "\n",
    "model_2 = GradientBoostingClassifier(n_estimators= 130, learning_rate=0.1, max_depth=2, min_samples_split=100, max_features=7, subsample = 0.9)\n",
    "grid_search = GridSearchCV(model_2, grid, cv = 3, verbose=2, n_jobs = -1)\n",
    "grid_result = grid_search.fit(X_train, y_train)\n",
    "print(f'Best Score: {grid_result.best_score_ * 100:.2f}%')\n",
    "print(f'Best Hyperparameters: {grid_result.best_params_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Best Score: 60.87%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assess feature importance from Gradient Boost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Feature Importance Score')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "feat_imp = pd.Series(model_2.feature_importances_, X.columns).sort_values(ascending=False)\n",
    "feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning for AdaBoost Classification\n",
    "\n",
    "https://medium.com/@chaudhurysrijani/tuning-of-adaboost-with-computational-complexity-8727d01a9d20 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "\n",
    "def get_models():\n",
    "\tmodels = dict()\n",
    "\t# explore depths from 1 to 10\n",
    "\tfor i in range(1,11):\n",
    "\t\t# define base model\n",
    "\t\tbase = DecisionTreeClassifier(max_depth=i)\n",
    "\t\t# define ensemble model\n",
    "\t\tmodels[str(i)] = AdaBoostClassifier(n_estimators=150, base_estimator=base)\n",
    "\treturn models\n",
    "\n",
    "# evaluate a given model using cross-validation\n",
    "def evaluate_model(model, X, y):\n",
    "\t# define the evaluation procedure\n",
    "\tcv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\t# evaluate the model and collect the results\n",
    "\tscores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "\treturn scores\n",
    "\n",
    "# get the models to evaluate\n",
    "models = get_models()\n",
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "for name, model in models.items():\n",
    "\t# evaluate the model\n",
    "\tscores = evaluate_model(model, X, y)\n",
    "\t# store the results\n",
    "\tresults.append(scores)\n",
    "\tnames.append(name)\n",
    "\t# summarize the performance along the way\n",
    "\tprint('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n",
      "[CV] END ....................................algorithm=SAMME; total time=   7.5s\n",
      "[CV] END ....................................algorithm=SAMME; total time=   8.0s\n",
      "[CV] END ....................................algorithm=SAMME; total time=   9.9s\n",
      "[CV] END ..................................algorithm=SAMME.R; total time=  10.7s\n",
      "[CV] END ..................................algorithm=SAMME.R; total time=   7.6s\n",
      "[CV] END ..................................algorithm=SAMME.R; total time=   7.5s\n",
      "Best Score: 60.98%\n",
      "Best Hyperparameters: {'algorithm': 'SAMME.R'}\n"
     ]
    }
   ],
   "source": [
    "n_estimators = [int(x) for x in np.linspace(start = 10, stop = 200, num = 20)]\n",
    "\n",
    "grid = {#'n_estimators': n_estimators,\n",
    "#        'learning_rate': [1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0],\n",
    "#        'algorithm': ['SAMME', 'SAMME.R']\n",
    "        }\n",
    "\n",
    "base = DecisionTreeClassifier(max_depth=1)\n",
    "model_3 = AdaBoostClassifier(n_estimators=150, base_estimator=base)\n",
    "\n",
    "grid_search = GridSearchCV(model_3, grid, cv = 3, verbose=2, n_jobs = -1)\n",
    "grid_result = grid_search.fit(X_train, y_train)\n",
    "print(f'Best Score: {grid_result.best_score_ * 100:.2f}%')\n",
    "print(f'Best Hyperparameters: {grid_result.best_params_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Best Score: 60.98%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "#### Confusion matrices and prediction probability:\n",
    "\n",
    "The predicted class probabilities of an input sample is computed as the weighted mean predicted class probabilities of the classifiers in the ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_proba_plot(clf, X, y, cv=5, no_bins=25, x_min=0.5, x_max=1, classifier=''):\n",
    "    '''\n",
    "    Histogram display of correcly predicted results against incorrectly given results given the outputed probability of the classifier.\n",
    "    \n",
    "    '''\n",
    "    y_dup = []\n",
    "    correct_guess_pred = []\n",
    "    incorrect_guess_pred = []\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=cv, shuffle=True)\n",
    "    y_pred_cv = cross_val_predict(clf, X, y, cv=skf)\n",
    "    y_pred_proba_cv = cross_val_predict(clf, X, y, cv=skf, method='predict_proba')\n",
    "    y_dup.append(list(y))\n",
    "    for i in range(len(y_pred_cv)):\n",
    "        if y_pred_cv[i] == list(y)[i]:\n",
    "            correct_guess_pred.append(max(y_pred_proba_cv[i]))\n",
    "        if y_pred_cv[i] != list(y)[i]:\n",
    "            incorrect_guess_pred.append(max(y_pred_proba_cv[i]))         \n",
    "    \n",
    "    bins = np.linspace(x_min, x_max, no_bins)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.hist(incorrect_guess_pred, bins, alpha=0.5, edgecolor='#1E212A', color='red', label='Incorrect Prediction')\n",
    "    ax.hist(correct_guess_pred, bins, alpha=0.5, edgecolor='#1E212A', color='green', label='Correct Prediction')\n",
    "    ax.legend()\n",
    "    ax.set_title(f'{classifier}', y=1, fontsize=16, fontweight='bold');\n",
    "    ax.set(ylabel='Number of Occurences',\n",
    "            xlabel='Prediction Probability')\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_confusion_matrix(clf, X, y, display_labels='', title='', cv=5):\n",
    "    '''\n",
    "    Function to plot confusion matrix given the result of cross-validation.\n",
    "    '''\n",
    "    \n",
    "    y_pred = cross_val_predict(clf, X, y, cv=cv)\n",
    "    cm_norm = confusion_matrix(y, y_pred, normalize='true')\n",
    "    fig = ConfusionMatrixDisplay(confusion_matrix=cm_norm, display_labels=display_labels)\n",
    "    fig.plot(cmap=plt.cm.Blues)\n",
    "    fig.ax_.set_title(title)\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = LogisticRegression(solver='lbfgs', penalty='l2', C=0.001)\n",
    "model_2 = GradientBoostingClassifier(n_estimators= 130, learning_rate=0.1, max_depth=2, min_samples_split=100, max_features=7, subsample = 0.9)\n",
    "base = DecisionTreeClassifier(max_depth=1)\n",
    "model_3 = AdaBoostClassifier(n_estimators=150, base_estimator=base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_lgr = pred_proba_plot(model_1, \n",
    "                            X, \n",
    "                            y,\n",
    "                            no_bins=40,\n",
    "                            x_min=0.5,\n",
    "                            classifier='Logistic Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "\n",
    "cross_val_confusion_matrix(model_1, X, y, \n",
    "                            display_labels=('other', 'win'), \n",
    "                            title='Logistic Regression Confusion Matrix', \n",
    "                            cv=skf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pairwise relationship plot with sns PairGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "res = {-1: \"Loss\",\n",
    "        1: \"Victory\",\n",
    "        0: \"Draw\"}\n",
    "\n",
    "df = df.replace({'Outcome':res})\n",
    "\n",
    "grid = sns.PairGrid(df, hue=\"Outcome\", palette='Set1', height=5)\n",
    "grid.map_diag(sns.histplot)\n",
    "grid.map_offdiag(sns.scatterplot)\n",
    "grid.add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross validation:\n",
    "\n",
    "Different cross validation techniques are used to see how they impact the output scores at each fold and to validate that the model is outputting consistent predictions (to satisfy the reliability of the model).\n",
    "\n",
    "https://goldinlocks.github.io/Time-Series-Cross-Validation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Fold CV: ['61.63%', '61.16%', '60.89%', '60.80%', '60.48%']\n",
      "Time Series CV: ['60.38%', '60.38%', '60.93%', '60.43%', '60.19%', '60.35%', '61.23%', '61.46%', '61.10%', '61.43%']\n",
      "Blocking Time Series CV: ['60.46%', '57.63%', '61.70%', '57.00%', '59.90%', '61.63%', '60.04%', '61.97%', '60.80%', '61.42%']\n"
     ]
    }
   ],
   "source": [
    "class BlockingTimeSeriesSplit():\n",
    "    \n",
    "    def __init__(self, n_splits):\n",
    "        self.n_splits = n_splits\n",
    "    \n",
    "    def get_n_splits(self, X, y, groups):\n",
    "        return self.n_splits\n",
    "    \n",
    "    def split(self, X, y=None, groups=None):\n",
    "        n_samples = len(X)\n",
    "        k_fold_size = n_samples // self.n_splits\n",
    "        indices = np.arange(n_samples)\n",
    "\n",
    "        margin = 0\n",
    "        for i in range(self.n_splits):\n",
    "            start = i * k_fold_size\n",
    "            stop = start + k_fold_size\n",
    "            mid = int(0.8 * (stop - start)) + start\n",
    "            yield indices[start: mid], indices[mid + margin: stop]\n",
    "\n",
    "cv_techniques = [\n",
    "    ('K-Fold CV', KFold(n_splits=5, random_state=1, shuffle=True)),\n",
    "    ('Time Series CV', TimeSeriesSplit(n_splits=10)),\n",
    "    ('Blocking Time Series CV', BlockingTimeSeriesSplit(n_splits=10))\n",
    "]\n",
    "\n",
    "model_1 = LogisticRegression(solver='lbfgs', penalty='l2', C=0.001)\n",
    "model_2 = GradientBoostingClassifier(n_estimators= 130, learning_rate=0.1, max_depth=2, min_samples_split=100, max_features=7, subsample = 0.9)\n",
    "base = DecisionTreeClassifier(max_depth=1)\n",
    "model_3 = AdaBoostClassifier(n_estimators=150, base_estimator=base)\n",
    "\n",
    "for cvs in cv_techniques:\n",
    "\n",
    "    accu = cross_val_score(model_2, X_train, y_train, cv=cvs[1], n_jobs=-1, scoring='accuracy')\n",
    "    accu = [f'{a * 100:.2f}%' for a in accu]\n",
    "    print(f'{cvs[0]}: {accu}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Significant difference:\n",
    "\n",
    "As the accuracy scores for different models are similar, is it necessary to determine if there is significance difference between these results.\n",
    "\n",
    "https://machinelearningmastery.com/statistical-significance-tests-for-comparing-machine-learning-algorithms/\n",
    "https://www.maartengrootendorst.com/blog/validate/\n",
    "\n",
    "    H0 = no significant difference between models\n",
    "    H1 = significant difference between models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wilcoxon signed-rank test: uses k accuracy scores of each model from k-fold cross-validation to test if the two samples differ significantly from each other. If p<0.05, reject the null hypothesis, H0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lgr = LogisticRegression(solver='lbfgs', penalty='l2', C=0.001)\n",
    "model_gbc = GradientBoostingClassifier(n_estimators= 130, learning_rate=0.1, max_depth=2, min_samples_split=100, max_features=7, subsample = 0.9)\n",
    "base = DecisionTreeClassifier(max_depth=1)\n",
    "model_abc = AdaBoostClassifier(n_estimators=150, base_estimator=base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import wilcoxon\n",
    "\n",
    "def wilcoxon_test(model_1, model_2):\n",
    "    \n",
    "    model_1.fit(X_train, y_train)\n",
    "    model_2.fit(X_train, y_train)\n",
    "    \n",
    "    model_1_scores = cross_val_score(model_1, X_train, y_train, cv=cv_techniques[0][1], n_jobs=-1, scoring='accuracy')\n",
    "    model_2_scores = cross_val_score(model_2, X_train, y_train, cv=cv_techniques[0][1], n_jobs=-1, scoring='accuracy')\n",
    "    \n",
    "    stat, p = wilcoxon(model_1_scores, model_2_scores, zero_method='zsplit')\n",
    "    \n",
    "    print(\n",
    "        f\"{model_1} vs {model_2}:\",\n",
    "        f\"Wilcoxon test: {stat, p}\"\n",
    "        )\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=0.001) vs GradientBoostingClassifier(max_depth=2, max_features=7, min_samples_split=100,\n",
      "                           n_estimators=130, subsample=0.9) Wilcoxon test: (2.0, 0.1875)\n",
      "LogisticRegression(C=0.001) vs AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),\n",
      "                   n_estimators=150) Wilcoxon test: (5.0, 0.625)\n",
      "GradientBoostingClassifier(max_depth=2, max_features=7, min_samples_split=100,\n",
      "                           n_estimators=130, subsample=0.9) vs AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),\n",
      "                   n_estimators=150) Wilcoxon test: (1.0, 0.125)\n"
     ]
    }
   ],
   "source": [
    "wilcoxon_test(model_lgr, model_gbc)\n",
    "wilcoxon_test(model_lgr, model_abc)\n",
    "wilcoxon_test(model_gbc, model_abc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "McNemar’s test: is a statistical test for paired nominal data. Specifically, it is used to compare the predictive accuracy of two models. McNemar's test is based on a 2x2 contingency table of the two model's predictions. From that table, we can calculate chi-squared (x²) which can be used to compute the p-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mlxtend.evaluate import mcnemar_table, mcnemar\n",
    "\n",
    "def mcnemar_test(model_1, model_2):\n",
    "    \n",
    "    model_1_predict = model_1.predict(X_test)\n",
    "    model_2_predict = model_2.predict(X_test)\n",
    "    \n",
    "    tb = mcnemar_table(y_target = y_test, \n",
    "                        y_model1 = model_1_predict, \n",
    "                        y_model2 = model_2_predict)\n",
    "    chi2, p = mcnemar(ary=tb, exact=True)\n",
    "    \n",
    "    print(f\"{model_1} vs {model_2}:\",\n",
    "        f'{accuracy_score(y_test, model_1_predict) * 100:.2f}%',\n",
    "        f'{accuracy_score(y_test, model_2_predict) * 100:.2f}%',\n",
    "        f'chi-squared: {chi2}',\n",
    "        f'p-value: {p}')\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=0.001) vs GradientBoostingClassifier(max_depth=2, max_features=7, min_samples_split=100,\n",
      "                           n_estimators=130, subsample=0.9): 60.27% 60.21% chi-squared: 1818 p-value: 0.7659107154719285\n",
      "LogisticRegression(C=0.001) vs AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),\n",
      "                   n_estimators=150): 60.27% 60.03% chi-squared: 1979 p-value: 0.2502882305859997\n",
      "GradientBoostingClassifier(max_depth=2, max_features=7, min_samples_split=100,\n",
      "                           n_estimators=130, subsample=0.9) vs AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),\n",
      "                   n_estimators=150): 60.21% 60.03% chi-squared: 967 p-value: 0.22595878362927932\n"
     ]
    }
   ],
   "source": [
    "mcnemar_test(model_lgr, model_gbc)\n",
    "mcnemar_test(model_lgr, model_abc)\n",
    "mcnemar_test(model_gbc, model_abc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "combined 5x2CV F-test: procedure to compare the performance of two models. We assume a significance threshold of α=0.05 for rejecting the null hypothesis that both algorithms perform equally well on the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.evaluate import combined_ftest_5x2cv\n",
    "\n",
    "def f_test(model_1, model_2):\n",
    "    \n",
    "    t, p = combined_ftest_5x2cv(estimator1 = model_1,\n",
    "                            estimator2 = model_2,\n",
    "                            X = X_train, y = y_train,\n",
    "                            random_seed=1)\n",
    "    \n",
    "    print(f\"{model_1} vs {model_2}:\",\n",
    "        f't statistic: {t: .3f}',\n",
    "        f'p value: %.3f {p: .3f}')\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=0.001) vs GradientBoostingClassifier(max_depth=2, max_features=7, min_samples_split=100,\n",
      "                           n_estimators=130, subsample=0.9) t statistic:  4.827 p value: %.3f  0.048\n",
      "LogisticRegression(C=0.001) vs AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),\n",
      "                   n_estimators=150) t statistic:  3.147 p value: %.3f  0.109\n",
      "GradientBoostingClassifier(max_depth=2, max_features=7, min_samples_split=100,\n",
      "                           n_estimators=130, subsample=0.9) vs AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),\n",
      "                   n_estimators=150) t statistic:  1.619 p value: %.3f  0.310\n"
     ]
    }
   ],
   "source": [
    "f_test(model_lgr, model_gbc)\n",
    "f_test(model_lgr, model_abc)\n",
    "f_test(model_gbc, model_abc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistical vs. Practical Significance\n",
    "\n",
    "Statistical significance refers to the unlikelihood that mean differences observed in the sample have occurred due to sampling error. Given a large enough sample, despite seemingly insignificant population differences, one might still find statistical significance. \n",
    "\n",
    "Practical significance looks at whether the difference is large enough to be of value in a practical sense, this is more subjective.\n",
    "\n",
    "https://towardsdatascience.com/comparing-machine-learning-models-statistical-vs-practical-significance-de345c38b42a\n",
    "\n",
    "Large sample size: means small differences can be detected with a hypothesis test and therefore those tiny deviations could be perceived as significant.\n",
    "\n",
    "mlxtend combined 5x2CV F-test takes in the full training set sample size (approx 7k samples) which is a very large sample. Therefore, while p<0.05 for the f-test, based on the other hypothesis testing and pratical significance we can make an intuitive decision that there is no significant difference between the models. "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c06b237160dce937b4e28b39103a0ec7f27cacf16c319f63cb1c746820d6a216"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('myenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
